{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b17077a",
   "metadata": {},
   "source": [
    "# WASP RL - Meeting 2 - Lab assignment on NAF  and SQL\n",
    "\n",
    "In this lab we implement and experiment with NAF and SQL, both algorithms we learned about in the course and both can deal with continuous state **and** action spaces. Both algorithms employ deep neural networks for function approximation. To do this assignment, you will need to work with PyTorch, a popular framework for developing neural networks. When forming groups, make sure that at least one group member if comfortable with PyTorch and its most common tensor operations.\n",
    "\n",
    "This notebook has been created specifically for this course and this session by *Finn Rietz* and *Johannes A. Stork*, we hope you enjoy the lab :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6ad1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control]==0.26.2 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy==1.23.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (1.23.0)\n",
      "Requirement already satisfied: matplotlib==3.1.2 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (3.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from gym[classic_control]==0.26.2) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from gym[classic_control]==0.26.2) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from gym[classic_control]==0.26.2) (6.6.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from gym[classic_control]==0.26.2) (2.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from matplotlib==3.1.2) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from matplotlib==3.1.2) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from matplotlib==3.1.2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from matplotlib==3.1.2) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[classic_control]==0.26.2) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib==3.1.2) (1.16.0)\n",
      "Requirement already satisfied: torch==1.12.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (1.12.0)\n",
      "Requirement already satisfied: torchaudio==0.12.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: torchvision==0.13.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: scipy==1.3.3 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (1.3.3)\n",
      "Requirement already satisfied: typing-extensions in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from torch==1.12.0) (4.5.0)\n",
      "Requirement already satisfied: numpy in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from torchvision==0.13.0) (1.23.0)\n",
      "Requirement already satisfied: requests in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from torchvision==0.13.0) (2.30.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from torchvision==0.13.0) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from requests->torchvision==0.13.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from requests->torchvision==0.13.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from requests->torchvision==0.13.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/coder/micromamba/envs/lab2/lib/python3.8/site-packages (from requests->torchvision==0.13.0) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "# install requirements if needed\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym[classic_control]==0.26.2 numpy==1.23.0 matplotlib==3.1.2 \n",
    "!{sys.executable} -m pip install torch==1.12.0 torchaudio==0.12.0 torchvision==0.13.0 scipy==1.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74086c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kde\n",
    "import random\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"torch device: {DEVICE}\")\n",
    "\n",
    "from buffer import ReplayBuffer\n",
    "from utils import presample_env, transform_action, smooth, obs_transform, OUNoise, rbf_kernel2\n",
    "from env_v2 import MultiGoalEnv\n",
    "from network import MLP\n",
    "from plotting import plot_loss, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bfefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eafad94",
   "metadata": {},
   "source": [
    "# NAF\n",
    "We start with NAF. From the paper, we know that the value $V(\\mathbf{x} | \\theta)$, the action $\\mathbf{\\mu(x|}\\theta)$, and the matrix entries $\\mathbf{L(x}|\\theta)$ are being approximated by neural networks. Its easiest to just have one a single neural network with multiple output heads for each quantity. As a first step, **adjust the code below so that the network outputs the desired values**. This involves making output layers for each quantity and implementing the equations for $Q(x,u)|\\theta)$ and $A(x, u |\\theta)$ in the forward pass of the neural network:\n",
    "![NAF quantities](notebook_imgs/NAF_quantities.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffe14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAFNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, layer_size):\n",
    "        super(NAFNetwork, self).__init__()\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc_0 = nn.Linear(state_size, layer_size)\n",
    "        self.fc_1 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc_2 = nn.Linear(layer_size, layer_size)\n",
    "        \n",
    "        # TODO: make output layers\n",
    "        # HINT: The number of entries in a (not-strictly) lower-triangular n x n matrix is: n * (n + 1) / 2\n",
    "        # n : number of actions\n",
    "        self.mu_head = nn.Linear(layer_size,action_size)\n",
    "        self.v_head = nn.Linear(layer_size,1)\n",
    "        self.mat_head = nn.Linear(layer_size,int((action_size + 1)* action_size/2))\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\"\n",
    "        Forward pass of Normalized Advantage Function\n",
    "        Returns the noisy exploration action, Q(s, a), V(s), and the greedy, non-noisy action\n",
    "        \"\"\"\n",
    "        # get latent representation\n",
    "        x = torch.relu(self.fc_0(state))\n",
    "        x = torch.relu(self.fc_1(x))\n",
    "        x = torch.relu(self.fc_2(x))\n",
    "        \n",
    "        # TODO: predict the action mu, the value v and the matrix entries\n",
    "        greedy_action = self.mu_head (x)\n",
    "        # a neural network that separately outputs a value function term V(x)\n",
    "        V = self.v_head(x)\n",
    "        # the advantage function term A(x, a) is from action_diff @ P @ action_diff, where the matrix entries in P are parametrized by a neural network\n",
    "        mat_head = self.mat_head(x)\n",
    "        lower_triangular_entries = torch.tanh(mat_head)\n",
    "\n",
    "        greedy_action = greedy_action.unsqueeze(-1)\n",
    "        \n",
    "        # TODO: calculate P\n",
    "        # create empty lower-triangular matrix\n",
    "        # init n x n matrix,\n",
    "        L = torch.zeros((state.shape[0], self.action_size, self.action_size)).to(DEVICE)\n",
    "        # get indices of lower-triangular entries (with diagonal) to parametrize P\n",
    "        L_indices = torch.tril_indices(row=self.action_size, col=self.action_size)#, offset=0)\n",
    "        # fill lower-triangular matrix with entries x\n",
    "        L[:, L_indices[0], L_indices[1]] = lower_triangular_entries\n",
    "        # with the diagonal terms exponentiated.\n",
    "        L.diagonal(dim1=1, dim2=2).exp_()  # make diagonal entries positive\n",
    "        # calculate P, P(x_j) = L(x_j)L(x_j)T, P is a state-dependent, positive-definite matrix\n",
    "        P = L @ L.transpose(2, 1)  # P = LL^T\n",
    "        \n",
    "        # if an action is given (during batch update), calculate Q\n",
    "        Q = None\n",
    "        if action is not None:\n",
    "            # calculate Advantage:\n",
    "            action_diff = (action.unsqueeze(-1) - greedy_action)\n",
    "            #print (\"shape of P: \", P.shape)\n",
    "            #print (\"shape of action_diff: \", action_diff.shape)\n",
    "            A = -0.5 * (action_diff.transpose(2, 1) @ P @ action_diff).squeeze(-1)  # Advantage\n",
    "            Q = A + V\n",
    "\n",
    "\n",
    "        # add noise to the greedy action, for exploration in continuous action space\n",
    "        # Initialize a random process for action exploration\n",
    "        # TODO: size of noise is (batch_size, action_dim)?\n",
    "        noise_dist = MultivariateNormal(greedy_action.squeeze(-1), torch.inverse(P))  \n",
    "        noisy_action = noise_dist.sample()\n",
    "        \n",
    "        # assert noisy_action.shape == greedy_action.shape, f\"noisy_action.shape: {noisy_action.shape}, greedy_action.shape: {greedy_action.shape}\"\n",
    "        noisy_action = torch.clamp(noisy_action, min = -2, max = 2)\n",
    "        \n",
    "        return noisy_action, Q, V, greedy_action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210dc190",
   "metadata": {},
   "source": [
    "Next, we make a class for the NAF agent, that will hold an instance of the above class for the neural network. The agent also implements an update method that updates the parameters of the neural network networka and a rollout method. Fortunately, the NAF update is straightforward, since it directly minimizes the TD error. **In the below cell, in the `update` method, compute the loss as described in the algorithm**. The update method alongside the rollout method essentially implement the entire NAF algorithm:\n",
    "![NAF algorithm](notebook_imgs/NAF_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAFAgent:\n",
    "    def __init__(self, n_states, n_actions, buffer, net_size, gamma, tau, lr):\n",
    "        \"\"\"\n",
    "        @param n_states: The dimensionality of the state space\n",
    "        @param n_actions: The dimensionality of the action space\n",
    "        @param buffer: An instance of a replay buffer\n",
    "        @param net_size: The width of the NAF network layers\n",
    "        @param gamma: The discount rate\n",
    "        @param tau: The soft/polyak target network update rate\n",
    "        @param lr: The learning rate\n",
    "        \"\"\"\n",
    "        self.buffer = buffer\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.update_counter = 0\n",
    "        self.GAMMA = gamma\n",
    "        self.TAU = tau\n",
    "        self.LR = lr\n",
    "        self.loss_hist = []\n",
    "\n",
    "        # main network\n",
    "        self.naf_net = NAFNetwork(\n",
    "            state_size=n_states,\n",
    "            action_size=n_actions,\n",
    "            layer_size=net_size,\n",
    "        )\n",
    "        self.naf_net.to(DEVICE)\n",
    "\n",
    "        # target network\n",
    "        self.target_net = NAFNetwork(\n",
    "            state_size=n_states,\n",
    "            action_size=n_actions,\n",
    "            layer_size=net_size,\n",
    "        )\n",
    "        self.target_net.to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.naf_net.state_dict())\n",
    "\n",
    "        self.optim = Adam(self.naf_net.parameters(), lr=self.LR)\n",
    "        \n",
    "    def rollout(self, env, episode, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Runs one episode and does a batch update after each step.\n",
    "        \"\"\"\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        def ensure_np_array(var):\n",
    "            if not isinstance(var, np.ndarray):\n",
    "                var = np.array(var)\n",
    "            return var\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            if mode == \"train\":\n",
    "                action, _, _, _ = self.naf_net(torch.from_numpy(obs).unsqueeze(0).to(DEVICE).to(torch.float32))\n",
    "            elif mode == \"eval\":\n",
    "                _, _, _, action = self.naf_net(torch.from_numpy(obs).to(DEVICE).to(torch.float32))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode 'mode' given!\")\n",
    "            \n",
    "            action = transform_action(action.squeeze(0), env)\n",
    "            # fix code:\n",
    "            action = ensure_np_array(action)\n",
    "            \n",
    "            new_obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if mode == \"train\":\n",
    "                self.buffer.add(obs, action, reward, new_obs, done)\n",
    "                self.update()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Total reward {mode} episode {episode}: {total_reward}\")\n",
    "                return total_reward\n",
    "            \n",
    "            obs = new_obs\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Updates the neural network based on a batch of experiences sampled from the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "        self.optim.zero_grad()\n",
    "        \n",
    "        # TODO: compute the loss\n",
    "        # This involves calling self.loss_fn(Q, y)\n",
    "        # ---------------------\n",
    "        \n",
    "        # Calculate predicted Q values and target Q values\n",
    "        _, predicted_q_values, _, _ = self.naf_net(states, actions)\n",
    "        _, _, _, next_actions = self.naf_net(next_states)\n",
    "        _, _, next_values, _ = self.target_net(next_states, next_actions)\n",
    "        target_q_values = rewards + self.GAMMA * next_values * (1 - dones)\n",
    "\n",
    "        loss = self.loss_fn(predicted_q_values, target_q_values)\n",
    "\n",
    "\n",
    "                \n",
    "        # ---------------------\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        self.loss_hist.append(loss.item())\n",
    "        self.update_counter += 1\n",
    "\n",
    "        # polyak/soft target network udpate\n",
    "        for target_param, current_param in zip(self.target_net.parameters(), self.naf_net.parameters()):\n",
    "            target_param.data.copy_(self.TAU * current_param.data + (1.0 - self.TAU) * target_param.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2dd2bb2",
   "metadata": {},
   "source": [
    "Now, we just need a method that initializes the agent and runs the main trianing loop . **In the next cell, don't change anything**. You can play with the hyperparameters later, but when we highly recommend that you don't change them for now, since the ones we put for you definitly work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625fe775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naf_algo(env): \n",
    "    # hyperparameters\n",
    "    TRAIN_EPISODES = 200\n",
    "    NET_SIZE = 64\n",
    "    GAMMA = 0.975\n",
    "    TAU = 0.0025\n",
    "    LR = 0.001\n",
    "    BATCH_SIZE = 128\n",
    "    \n",
    "    buffer = ReplayBuffer(\n",
    "        buffer_size=int(1e6),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    agent = NAFAgent(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.shape[0],\n",
    "        buffer,\n",
    "        NET_SIZE,\n",
    "        GAMMA,\n",
    "        TAU,\n",
    "        LR\n",
    "    )\n",
    "    \n",
    "    # populate replay buffer with 10k random transitions\n",
    "    presample_env(env, agent.buffer, 10000)\n",
    "\n",
    "    # main loop\n",
    "    training_rewards = []\n",
    "    eval_rewards = []\n",
    "    eval_eps = []\n",
    "    for episode in range(TRAIN_EPISODES):\n",
    "        tr = agent.rollout(env, episode)\n",
    "        training_rewards.append(tr)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            er = agent.rollout(env, episode, mode=\"eval\")\n",
    "            eval_rewards.append(er)\n",
    "            eval_eps.append(episode)\n",
    "            \n",
    "    plot_loss(agent.loss_hist)\n",
    "    plot_reward(training_rewards, eval_rewards, eval_eps)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30ffe96f",
   "metadata": {},
   "source": [
    "# Testing NAF on Pendulum environment\n",
    "Now, with all of this (partially boilerplate) code out of the way, its time to test our NAF agent. We use the [OpenAI Gym Pendulum environment](https://mgoulao.github.io/gym-docs/environments/classic_control/pendulum/), which is arguably one of the easiest, continuous action space environments. The agent should reach about -250 reward relatively quickly, if you've implemented everything correctly. There can still be quite some variation (i.e. between 0 and -700), depending on the random initialization of the pendulum..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad3fc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presampling done\n",
      "Total reward train episode 0: -1668.1896728146537\n",
      "Total reward eval episode 0: -1160.2362152114936\n",
      "Total reward train episode 1: -1356.5270466455863\n",
      "Total reward train episode 2: -1075.0358263143273\n",
      "Total reward train episode 3: -1654.6138690637133\n",
      "Total reward train episode 4: -1361.7501228394467\n",
      "Total reward train episode 5: -1758.5984457731643\n",
      "Total reward eval episode 5: -1869.5299885127217\n",
      "Total reward train episode 6: -1642.8950921143419\n",
      "Total reward train episode 7: -1412.283607684915\n",
      "Total reward train episode 8: -1744.2499270721214\n",
      "Total reward train episode 9: -1710.424139637102\n",
      "Total reward train episode 10: -1409.8701381408894\n",
      "Total reward eval episode 10: -1494.8752464495624\n",
      "Total reward train episode 11: -1505.038088452287\n",
      "Total reward train episode 12: -1379.4171621170244\n",
      "Total reward train episode 13: -1058.7325202204531\n",
      "Total reward train episode 14: -1288.569463639615\n",
      "Total reward train episode 15: -1065.381265102346\n",
      "Total reward eval episode 15: -964.2837541578459\n",
      "Total reward train episode 16: -1087.6511565779015\n",
      "Total reward train episode 17: -927.8074831579736\n",
      "Total reward train episode 18: -1045.226037541726\n",
      "Total reward train episode 19: -1064.9259813812716\n",
      "Total reward train episode 20: -1080.2568553138315\n",
      "Total reward eval episode 20: -269.5457293828885\n",
      "Total reward train episode 21: -558.3013797267566\n",
      "Total reward train episode 22: -901.9017299137013\n",
      "Total reward train episode 23: -789.3667096678023\n",
      "Total reward train episode 24: -928.6432357176235\n",
      "Total reward train episode 25: -455.0096671890084\n",
      "Total reward eval episode 25: -130.28464608978373\n",
      "Total reward train episode 26: -787.7470858714559\n",
      "Total reward train episode 27: -384.28715018367706\n",
      "Total reward train episode 28: -615.6408353699895\n",
      "Total reward train episode 29: -405.02486058381714\n",
      "Total reward train episode 30: -1020.9243785611651\n",
      "Total reward eval episode 30: -132.38263563109254\n",
      "Total reward train episode 31: -654.078875954886\n",
      "Total reward train episode 32: -776.4874062495288\n",
      "Total reward train episode 33: -398.20332177252476\n",
      "Total reward train episode 34: -788.0697408980939\n",
      "Total reward train episode 35: -527.4643406990824\n",
      "Total reward eval episode 35: -258.6904321765585\n",
      "Total reward train episode 36: -932.7096697118842\n",
      "Total reward train episode 37: -770.1934313278625\n",
      "Total reward train episode 38: -644.327021798216\n",
      "Total reward train episode 39: -543.7915776408129\n",
      "Total reward train episode 40: -821.7422079856753\n",
      "Total reward eval episode 40: -1491.941968256291\n",
      "Total reward train episode 41: -633.2902623892205\n",
      "Total reward train episode 42: -651.3764704104286\n",
      "Total reward train episode 43: -516.4659413519679\n",
      "Total reward train episode 44: -260.382285679736\n",
      "Total reward train episode 45: -793.60028111251\n",
      "Total reward eval episode 45: -125.93123414317392\n",
      "Total reward train episode 46: -1492.1000012672314\n",
      "Total reward train episode 47: -646.3896899123183\n",
      "Total reward train episode 48: -6.536687022585717\n",
      "Total reward train episode 49: -865.6127868120407\n",
      "Total reward train episode 50: -253.51434231355515\n",
      "Total reward eval episode 50: -131.17547589636567\n",
      "Total reward train episode 51: -633.0787663984848\n",
      "Total reward train episode 52: -903.7489080567373\n",
      "Total reward train episode 53: -135.1894470357467\n",
      "Total reward train episode 54: -856.4331528660546\n",
      "Total reward train episode 55: -643.0909915181554\n",
      "Total reward eval episode 55: -493.47399706907953\n",
      "Total reward train episode 56: -873.1377925127721\n",
      "Total reward train episode 57: -759.062672073423\n",
      "Total reward train episode 58: -879.6059031741769\n",
      "Total reward train episode 59: -485.4284442922222\n",
      "Total reward train episode 60: -709.9881640369598\n",
      "Total reward eval episode 60: -2.4907335346849275\n",
      "Total reward train episode 61: -870.5769155150931\n",
      "Total reward train episode 62: -1007.3643161686067\n",
      "Total reward train episode 63: -965.5872816146069\n",
      "Total reward train episode 64: -637.3137313596746\n",
      "Total reward train episode 65: -907.3748692358819\n",
      "Total reward eval episode 65: -253.18654947608232\n",
      "Total reward train episode 66: -868.4358807675005\n",
      "Total reward train episode 67: -903.4887981455819\n",
      "Total reward train episode 68: -867.6339665852142\n",
      "Total reward train episode 69: -984.9432803409202\n",
      "Total reward train episode 70: -838.5904239655256\n",
      "Total reward eval episode 70: -263.32779396081725\n",
      "Total reward train episode 71: -740.6156061027064\n",
      "Total reward train episode 72: -626.3395996622725\n",
      "Total reward train episode 73: -997.7114810453559\n",
      "Total reward train episode 74: -1492.8474189825895\n",
      "Total reward train episode 75: -976.1513959736675\n",
      "Total reward eval episode 75: -267.32431570672003\n",
      "Total reward train episode 76: -902.8403231301322\n",
      "Total reward train episode 77: -991.697708420421\n",
      "Total reward train episode 78: -1081.4042499455313\n",
      "Total reward train episode 79: -877.2317994869178\n",
      "Total reward train episode 80: -931.1841042468066\n",
      "Total reward eval episode 80: -488.83716537058865\n",
      "Total reward train episode 81: -883.1632576541448\n",
      "Total reward train episode 82: -979.4182300293983\n",
      "Total reward train episode 83: -997.412886488947\n",
      "Total reward train episode 84: -787.2860497741351\n",
      "Total reward train episode 85: -987.4417208847757\n",
      "Total reward eval episode 85: -400.94270642258067\n",
      "Total reward train episode 86: -899.9925189122962\n",
      "Total reward train episode 87: -990.7234960211449\n",
      "Total reward train episode 88: -923.7676772768978\n",
      "Total reward train episode 89: -885.059387158385\n",
      "Total reward train episode 90: -868.728814848648\n",
      "Total reward eval episode 90: -860.42811601092\n",
      "Total reward train episode 91: -897.2508958852081\n",
      "Total reward train episode 92: -1001.4733875389966\n",
      "Total reward train episode 93: -875.1206416931078\n",
      "Total reward train episode 94: -892.336762790074\n",
      "Total reward train episode 95: -621.8137138992304\n",
      "Total reward eval episode 95: -261.7202533278325\n",
      "Total reward train episode 96: -880.6227063176643\n",
      "Total reward train episode 97: -627.6365470575264\n",
      "Total reward train episode 98: -1493.9323848972683\n"
     ]
    }
   ],
   "source": [
    "pendulum_env = gym.make(\"Pendulum-v1\")\n",
    "naf_agent = naf_algo(pendulum_env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fa40531",
   "metadata": {},
   "source": [
    "### Continue with the next part of the assignment only when your NAF implementation maintains between 0 and -700 reward on the pendulum environment!\n",
    "Once the NAF agent reaches acceptable performance on the pendulum environment, explore the effect of exploration noise. In the NAF paper in section 8.2, the authors describe how to use the matrix $P$ from the advantage term for adaptive noise generation. Assuming you used simple Gaussian noise so far, go back to the the forward pass of the neural network and implement adaptive noise instead. Do you observe a considerable difference on the Pendulum environment when using adaptive noise?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0594968",
   "metadata": {},
   "source": [
    "# Multigoal environment\n",
    "Now, we turn to a more interesting but still toy example, the \"multigoal\" environment introduced in the Soft Q-Learning paper. This environment features four goals (the stars in the below image) the agent can navigate to. The observation is the current position, the actions are 2D velocities to apply to the agent. The agent always starts at the center (plus some small random offset). The reward is the distance to the closest goal plus a small cost proportional to the squarred action sum. Consider below the plot of the reward function evaluated at a fine grid of locations for further intuition:\n",
    "![multigoal env](notebook_imgs/multigoal-env-reward.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36d0dda3",
   "metadata": {},
   "source": [
    "# TASK: Draw on a paper a [contour plot](https://se.mathworks.com/help/examples/graphics/win64/DisplayContourLabelsExample_01.png) of what you think NAF's value function will look like, evaluate at 2D locations in the environment as done with the reward function above. Keep the paper for later comparison with the actual value function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b370fb9",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cdc73f8",
   "metadata": {},
   "source": [
    "# TASK: Also draw the trajectories that you think NAF's policy will generate. Keep the paper for later comparison with actual trajectories generated by the policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31013762",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe717f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multigoal_env = MultiGoalEnv()\n",
    "naf_agent = naf_algo(multigoal_env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da896ada",
   "metadata": {},
   "source": [
    "# Analyze the reward and loss plots. Explain the result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2ca3196",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "373ed727",
   "metadata": {},
   "source": [
    "# Visualizing the value function\n",
    "In addition to the the loss plot and reward plot above, it can be helpfull to visualize the learned value function and or policy, to understand better what the agent learned. This is often straightforward in 2D environments, since a small 2D grid is still managable in terms of compute. As such, **visualize the value function learned by the NAF agent on the multigoal environment**. Does NAF's learned value function look similar to what you drew on the paper? Can you explain why the learned value function looks the way it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb618e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa9c3b2",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7805e172",
   "metadata": {},
   "source": [
    "# Visualizing trajectories\n",
    "In addition to the value function, we can inspect the trajectories generated by the policy. **Collect, plot and analyze some (10 - 50) trajectories from the policy**. Are the agent's trajectory similar to what you drew on the paper? Explain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "multigoal_env.init_sigma = 0.4  # we increase the reset noise, so we get a bit more diverse starting positions...\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11de1f1a",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4153ee7f",
   "metadata": {},
   "source": [
    "# Soft Q-Learning\n",
    "\n",
    "\n",
    "We now turn to the second algorithm we learned about that can deal with continuous state and action spaces: Soft Q-learning (SQL). SQL is based on two neural networks, one for the $Q$-function and one for the policy. These are plain MLPs, hence this time you don't have to implement the neural network. However, you must initialize these networks with the correct input and output sizes. Thus, **in the `init` method in the next cell, set the `q_in_size`, `q_out_size`, `asvgd_noise_size`, `pi_in_size`, and `pi_out_size` variables**. These are passed to the respective neural networks initalizations.\n",
    "\n",
    "Further, you will implement the update of the $Q$-function approximating neural network. This is relatively straightforward and based on the TD-error. **In the next cell, in the `td_update` method, calculate the loss for the $Q$-approximating network**. For this, you need to implement and consider the following equations. The soft value function:\n",
    "![sql_vsoft](notebook_imgs/SQL_v_soft.png)\n",
    "The soft value function's empirical estimate:\n",
    "![sql_empirical_vsoft](notebook_imgs/SQL_empirical_v_soft.png)\n",
    "And the minimization objective:\n",
    "![sql_JQ](notebook_imgs/SQL_jq.png)\n",
    "Note, that $\\hat{Q}^{\\bar\\theta}_{\\text{soft}} = r_t + \\gamma \\mathbb{E}_{s_{t+1}\\sim p_s}[ V^{\\bar\\theta}_{\\text{soft}}(s_{t+1}) ]$ and that $\\bar\\theta$ refers to the target network parameter.\n",
    "\n",
    "We implement the update of the policy network for you, because itrelies on a method not covered in the course, the Amortized Stein Variational Gradient Descent (ASVGD). ASVGD has gained increasing popularity in SOTA research and provides an interesting alternative to sampling methods like Markov Chain Monte Carlo or Metropolis Hastings. ASVGD moves a set of random particles such that, after convergence, they act like samples from the distribution of interest, and only requires access to the unnormalized density. You can take a look how this is implemented and how it updates the policy network in the `asvgd_update` method. If you are interested, we encourage you to take a look at the official [project website](https://www.cs.utexas.edu/~qlearning/project.html?p=svgd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLAgent:\n",
    "    \"\"\"\n",
    "    SQL agent, https://arxiv.org/pdf/1702.08165.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_states,\n",
    "                 n_actions,\n",
    "                 buffer,\n",
    "                 net_size,\n",
    "                 gamma,\n",
    "                 q_lr,\n",
    "                 asvgd_lr,\n",
    "                 hard_freq,\n",
    "                 reward_scale,\n",
    "                 n_particles,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        @param n_states: The dimensionality of the state space\n",
    "        @param n_actions: The dimensionality of the action space\n",
    "        @param buffer: An instance of a replay buffer\n",
    "        @param net_size: The width of the NAF network layers\n",
    "        @param gamma: The discount rate\n",
    "        @param q_lr: The learning rate for the q network\n",
    "        @param asvgd_lr: The learnign rate for the policy network\n",
    "        @param hard_freq: The hard update frequency of the q target network\n",
    "        @param reward_scale: The scale of the reward signal\n",
    "        @param n_particles: The number of particles used by ASVGD\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.buffer = buffer\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        self.q_update_counter = 0\n",
    "        self.GAMMA = gamma\n",
    "        self.HARD_FREQ = hard_freq\n",
    "        self.loss_hist = []\n",
    "        self.reward_scale = reward_scale\n",
    "        self.noise_gen = OUNoise(n_actions)\n",
    "        self.n_particles = n_particles\n",
    "        \n",
    "        #  TODO: set these to the right values (these are all integers...)\n",
    "        # ---------------------\n",
    "        q_in_size = None\n",
    "        q_out_size = None\n",
    "        asvgd_noise_size = None\n",
    "        pi_in_size = None\n",
    "        pi_out_size = None\n",
    "        # ---------------------\n",
    "\n",
    "        # Q network\n",
    "        self.q_net = MLP(\n",
    "            in_size=q_in_size,\n",
    "            out_size=q_out_size,\n",
    "            layer_size=net_size,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # target Q network\n",
    "        self.target_q_net = MLP(\n",
    "            in_size=q_in_size,\n",
    "            out_size=q_out_size,\n",
    "            layer_size=net_size,\n",
    "        ).to(DEVICE)\n",
    "        self.target_q_net.to(DEVICE)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        # policy sampling network\n",
    "        self.pi_net = MLP(\n",
    "            in_size=pi_in_size,\n",
    "            out_size=pi_out_size,\n",
    "            layer_size=net_size,\n",
    "        ).to(DEVICE)\n",
    "        self.asvgd_noise_size = asvgd_noise_size\n",
    "\n",
    "        self.q_optim = Adam(self.q_net.parameters(), lr=q_lr, weight_decay=0.01)\n",
    "        self.asvgd_optim = Adam(self.pi_net.parameters(), lr=asvgd_lr)\n",
    "\n",
    "    def td_update(self):\n",
    "        \"\"\"\n",
    "        Updates the Q network based on a batch of experiences drawn uniformly from the replay buffer\n",
    "        \"\"\"\n",
    "        if len(self.buffer.memory) < self.buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "        self.q_optim.zero_grad()\n",
    "\n",
    "        # predict q for batch\n",
    "        q = self.q_net.forward(torch.cat((states, actions), dim=1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO, calculate the 'target' aka 'desired' value J_Q(\\theta), so that we can minimize the loss \n",
    "            # between q and the target\n",
    "            # ---------------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            target = None\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # ---------------------\n",
    "        \n",
    "        \n",
    "        loss = self.loss_fn(q, target)\n",
    "        loss.backward()\n",
    "        self.q_optim.step()\n",
    "        self.loss_hist.append(loss.item())\n",
    "\n",
    "        self.q_update_counter += 1\n",
    "\n",
    "        # hard target network update\n",
    "        if self.q_update_counter % self.HARD_FREQ == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "            \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Helper function that constructs the input to the policy network.\n",
    "        This is done slightly differently depending on on the shape of the state tensor.\n",
    "        \"\"\"\n",
    "        if state.shape[0] == 1:\n",
    "            # environment interaction\n",
    "            zeta = torch.rand(self.n_particles, self.asvgd_noise_size).to(DEVICE)\n",
    "            state = state.repeat(self.n_particles, 1)\n",
    "            inp = torch.cat((state, zeta), dim=1)\n",
    "        else:\n",
    "            # batch update\n",
    "            zeta = torch.rand(state.shape[0], self.n_particles, self.asvgd_noise_size).to(DEVICE)\n",
    "            state = state.repeat(self.n_particles, 1, 1).movedim(1, 0)\n",
    "            inp = torch.cat((state, zeta), dim=2)\n",
    "\n",
    "        action = self.pi_net.forward(inp.to(torch.float32))\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def rollout(self, env, episode, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        Runs one episode and does a batch update after each step.\n",
    "        \"\"\"\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            a = self.act(torch.from_numpy(obs).unsqueeze(0).to(torch.float32).to(DEVICE))\n",
    "            o_stack = obs_transform(torch.from_numpy(obs).unsqueeze(0), self.n_particles).to(torch.float32).to(DEVICE)\n",
    "            q = self.q_net.forward(torch.cat((o_stack, a), dim=1))\n",
    "            ind = torch.argmax(q)\n",
    "            action = a[ind]\n",
    "\n",
    "            if mode == \"train\":\n",
    "                action += torch.from_numpy(self.noise_gen.sample()).to(DEVICE)\n",
    "\n",
    "            action = transform_action(action, env)\n",
    "            new_obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if mode == \"train\":\n",
    "                self.buffer.add(obs, action, reward, new_obs, done)\n",
    "                self.td_update()\n",
    "                self.asvgd_update()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Total reward {mode} episode {episode}: {total_reward}\")\n",
    "                return total_reward\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "    def asvgd_update(self):\n",
    "        \"\"\"\n",
    "        Updates the policy network using the ASVGD method on a batch of experiences drawn uniformly from the \n",
    "        replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.buffer.memory) < self.buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "\n",
    "        # as described in the appendix of SQL paper, we need two sets of actions\n",
    "        actions = self.act(states)\n",
    "        assert actions.shape == (states.shape[0], self.n_particles, self.n_actions)\n",
    "\n",
    "        fixed_actions = self.act(states)\n",
    "        fixed_actions = fixed_actions.detach()\n",
    "        fixed_actions.requires_grad = True\n",
    "\n",
    "        # target for sampler is q, aka unnormalized policy density, thanks to maximum entropy framework\n",
    "        state_stack = obs_transform(states, self.n_particles).to(DEVICE)\n",
    "        asvgd_target = self.q_net(torch.cat((state_stack, fixed_actions), dim=2))\n",
    "        log_p = asvgd_target\n",
    "\n",
    "        grad_log_p = torch.autograd.grad(log_p.sum().to(DEVICE), fixed_actions.to(DEVICE))[0]\n",
    "        grad_log_p = grad_log_p.unsqueeze(1)\n",
    "\n",
    "        kappa, kappa_grad = rbf_kernel2(actions, fixed_actions)\n",
    "        \n",
    "        # eq 13 in paper, stein gradient\n",
    "        actions_grad = (1/self.n_particles) * torch.sum(kappa * grad_log_p + kappa_grad, dim=1)\n",
    "        actions_grad.to(DEVICE)\n",
    "\n",
    "        self.asvgd_optim.zero_grad()\n",
    "        torch.autograd.backward(-actions, grad_tensors=actions_grad)  # this implements eq 14, chain rule backprop\n",
    "        self.asvgd_optim.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84378d8d",
   "metadata": {},
   "source": [
    "With the update methods taken care of, as before, the only thing that is missing is the main loop. Also as before, we high suggest you don't change any of the hyperparameters until your SQL implementation is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_algo(env, reward_scale, n_particles):\n",
    "    # hyperparameters\n",
    "    TRAIN_EPISODES = 200\n",
    "    BATCH_SIZE = 64\n",
    "    NET_SIZE = 128\n",
    "    GAMMA = 0.99\n",
    "    Q_LR = 0.001\n",
    "    PI_LR = 0.0001\n",
    "    HARD_FREQ = 1000\n",
    "    \n",
    "    buffer = ReplayBuffer(\n",
    "        buffer_size=int(1e6),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    agent = SQLAgent(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.shape[0],\n",
    "        buffer,\n",
    "        NET_SIZE,\n",
    "        GAMMA,\n",
    "        Q_LR,\n",
    "        PI_LR,\n",
    "        HARD_FREQ,\n",
    "        reward_scale,\n",
    "        n_particles\n",
    "    )\n",
    "    \n",
    "    # populate replay buffer with 10k random transitions\n",
    "    presample_env(env, agent.buffer, 10000)\n",
    "\n",
    "    # main loop\n",
    "    training_rewards = []\n",
    "    eval_rewards = []\n",
    "    eval_eps = []\n",
    "    for episode in range(TRAIN_EPISODES):        \n",
    "        tr = agent.rollout(env, episode)\n",
    "        training_rewards.append(tr)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            er = agent.rollout(env, episode, mode=\"eval\")\n",
    "            eval_rewards.append(er)\n",
    "            eval_eps.append(episode)\n",
    "            \n",
    "    plot_loss(agent.loss_hist)\n",
    "    plot_reward(training_rewards, eval_rewards, eval_eps)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1188095",
   "metadata": {},
   "source": [
    "# Testing SQL on Pendulum environment\n",
    "As before, we first evaluate our implementation on the pendulum environment. The algorithm should be able to reach good performance (between 0 and -700) reward within 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59120c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pendulum_env = gym.make(\"Pendulum-v1\")\n",
    "sql_agent = sql_algo(pendulum_env, reward_scale=10, n_particles=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc63bdf",
   "metadata": {},
   "source": [
    "# Testin SQL on Multigoal environment\n",
    "Once you have verified your SQL implementation, you can continue and test it on the multigoal environment.\n",
    "\n",
    "## Again, draw what you think the value function learned by SQL will look like. Also draw the trajectories you think SQL will generate.\n",
    "Again keep the paper for later comparison with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca46cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multigoal_env = MultiGoalEnv()\n",
    "sql_agent = sql_algo(multigoal_env, reward_scale=1, n_particles=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61bffd1e",
   "metadata": {},
   "source": [
    "### Analyze the plots. Does SQL solve the multigoal environment? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7613a1f4",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67240080",
   "metadata": {},
   "source": [
    "# Visualizing the value function\n",
    "As before, **visualze the learned value function of the SQL agent**. How does it relate to the reward landscape we saw earlier? How does it compare to the value function learned by NAF? How does it compare to the value function you drew on the paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad69885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26913d7a",
   "metadata": {},
   "source": [
    "# Visualizing the policy\n",
    "Now, lets also try to **visualize the policy network at interesting points in the environment**. These points coulde be $(0, 0), (\\pm 2, 0),$ or $(\\pm2.5, \\pm2.5)$. Construct the observation manually, feed it into the policy network and visualize the particles. Bonus points if you run KDE on the generated particles ;)\n",
    "What do you observe? How does the plot of the particles and the KDE explain the shortcomming of the NAF algorithm on the multigoal environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f1b9cf7",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81bf85fc",
   "metadata": {},
   "source": [
    "# Visualizing trajectories\n",
    "As we did before with NAF, lets explore trajectories taken by the learned SQL policy. **Collect, plot and analyze some (10 - 50) trajectories generated by the SQL agent**. Are the trajectories similar to what you drew on the paper? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e183aeaa",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "707f4a29",
   "metadata": {},
   "source": [
    "# Optional tasks:\n",
    "If you are done with everything and there is time left, we encourage you to do some (or all) of the following tasks:\n",
    "+ Play with hyperparameters: For SQL, particularly interesting might be the reward scale, the number of particles used by the policy network with ASVGD, and the learning rates `Q_LR` and `PI_LR` (they might have an unespected relationship).\n",
    "+ Generally, varying the batch size, the network size, the loss functions, and the optimizers target update rate `TAU` or hard update frequncy `HARD_FREQ` can have intersting effects. A poor value can often render the RL algorithm completely incapable of learning a task, while a good value can speed up and stabilize training considerably. Try varying those parameters and note the effects.\n",
    "+ You can implement different explortation mechanisms. For example, the SQL implementation uses OU noise, which is a a form of temporally correlated noise and a rather powerful mechanism. How does SQL perform with, for example, epsilon greedy or Gaussian noise? In the same way, perhaps NAF can be improved by using OU noise instead of Gaussian or adaptive noise?\n",
    "+ Lastly, given the somewhat complex ASVGD update mechanism in the SQL policy update, you can implement a simpler method like Metropolis Hastings or Hamiltonian Monte Carlo to sample from the policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
